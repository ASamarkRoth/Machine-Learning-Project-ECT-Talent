{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load code for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import json\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import sklearn.preprocessing\n",
    "\n",
    "FEATURES = 0\n",
    "TARGETS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_h5_colab():\n",
    "    \"\"\"Loads in the AT-TPC data.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple of the form ((train_features, train_targets), (test_features, test_targets))\n",
    "    \"\"\"\n",
    "\n",
    "    data_origin = 'https://github.com/CompPhysics/MachineLearningMSU/raw/master/Day2_materials/data/real-attpc-events.h5'\n",
    "    \n",
    "    path = tf.keras.utils.get_file('xyimages.h5', origin=data_origin)\n",
    "    \n",
    "    h5 = h5py.File(path, 'r')\n",
    "\n",
    "    train_features = h5['train_features'][:]\n",
    "    train_targets = h5['train_targets'][:]\n",
    "    test_features = h5['test_features'][:]\n",
    "    test_targets = h5['test_targets'][:]\n",
    "    \n",
    "    return (train_features, train_targets), (test_features, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train, log_dir, epochs=10, batch_size=32, data_combine=False, rebalance=False, binary=False, lr=0.00001, decay=0., validation_split=0.15, freeze=False,\n",
    "         examples_limit=-1, seed=71, reverse_labels=True, validation_size=None, use_dropout=True):\n",
    "    \"\"\"This function will train a CNN classifier using the VGG16 architecture with ImageNet weights.\"\"\"\n",
    "    #assert data.endswith('.h5'), 'train_path must point to an HDF5 file'\n",
    "\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "\n",
    "    # Set random seeds\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    ## Load data\n",
    "    #if data_combine:\n",
    "    #    a, b = load_image_h5(data, categorical=True, binary=binary, reverse_labels=reverse_labels)\n",
    "    #    train = np.concatenate([a[FEATURES], b[FEATURES]], axis=0), np.concatenate([a[TARGETS], b[TARGETS]], axis=0)\n",
    "    #else:\n",
    "    #    train, _ = load_image_h5(data, categorical=False, binary=binary, reverse_labels=reverse_labels)\n",
    "        \n",
    "    #train = sklearn.preprocessing.StandardScaler().fit_transform(train)\n",
    "\n",
    "    print(\"TARGETS shape:\", len(train),train[TARGETS].shape)\n",
    "    print(\"FEATURES shape:\", len(train),train[FEATURES].shape, train[FEATURES].shape[1:])\n",
    "    #num_categories = train[TARGETS].shape[1]\n",
    "    num_categories = 1\n",
    "\n",
    "\n",
    "    # Build model\n",
    "    vgg16_base = tf.keras.applications.VGG16(include_top=False, input_shape=train[FEATURES].shape[1:], weights='imagenet')\n",
    "    net = vgg16_base.output\n",
    "    net = tf.keras.layers.Flatten()(net)\n",
    "    net = tf.keras.layers.Dense(256, activation=tf.nn.relu)(net)\n",
    "    if use_dropout:\n",
    "        net = tf.keras.layers.Dropout(0.5)(net)\n",
    "    preds = tf.keras.layers.Dense(num_categories, activation=tf.nn.sigmoid)(net) \n",
    "    model = tf.keras.Model(vgg16_base.input, preds)\n",
    "\n",
    "    # Freeze convolutional layers if needed\n",
    "    if freeze:\n",
    "        for layer in model.layers[:-4]:\n",
    "            layer.trainable = False\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(lr=lr, decay=decay)\n",
    "    loss = 'binary_crossentropy'# if num_categories == 2 else 'categorical_crossentropy'\n",
    "\n",
    "    #print(\"Loss:\", loss)\n",
    "\n",
    "    model.compile(loss=loss,\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    print(model.summary())\n",
    "\n",
    "    #os.makedirs(os.path.join(log_dir, 'ckpt'), exist_ok=True)\n",
    "    #ckpt_path = os.path.join(log_dir, 'ckpt', 'epoch-{epoch:02d}.h5')\n",
    "    \n",
    "    log_run = \"freeze{}_dropout{}_lr{}_decay{}_samples{}\".format(freeze, use_dropout, lr, decay, examples_limit)\n",
    "    #print(\"Log for run:\", log_run)\n",
    "    \n",
    "    log_dir = os.path.join(log_dir, log_run, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    print(\"\\nWriting fits to:\", log_dir)\n",
    "    ckpt_path = os.path.join(log_dir, 'epoch-{epoch:02d}.h5')\n",
    "    print(\"Checkpoint path:\", ckpt_path, \"\\n\")\n",
    "\n",
    "    # Get class weights\n",
    "    if rebalance:\n",
    "        targets_argmax = np.argmax(train[TARGETS], axis=1)\n",
    "        class_weight = compute_class_weight('balanced', np.unique(targets_argmax), targets_argmax)\n",
    "        class_weight = dict(enumerate(class_weight))\n",
    "    else:\n",
    "        class_weight = None\n",
    "\n",
    "    # Setup checkpoint callback\n",
    "    ckpt_callback = tf.keras.callbacks.ModelCheckpoint(ckpt_path,\n",
    "                                                       save_weights_only=False,\n",
    "                                                       save_frequency=1,\n",
    "                                                       save_best_only=False,\n",
    "                                                       monitor='val_loss')\n",
    "\n",
    "    # Setup TensorBoard callback\n",
    "    tb_callback = tf.keras.callbacks.TensorBoard(log_dir)\n",
    "\n",
    "    val = None\n",
    "\n",
    "    if validation_size is not None:\n",
    "        if validation_size >= train[TARGETS].shape[0]:\n",
    "            raise ValueError('The given validation size must be smaller than the size of the training set ({}).'.format(\n",
    "                train[TARGETS].shape[0]))\n",
    "        val = train[FEATURES][-validation_size:], train[TARGETS][-validation_size:]\n",
    "        train = train[FEATURES][:-validation_size], train[TARGETS][:-validation_size]\n",
    "\n",
    "    if examples_limit == -1:\n",
    "        examples_limit = train[TARGETS].shape[0]\n",
    "\n",
    "    if examples_limit > train[TARGETS].shape[0]:\n",
    "        warnings.warn('`examples_limit` is larger than the number of examples in the training set. The entire training '\n",
    "                      'set will be used.')\n",
    "        examples_limit = train[TARGETS].shape[0]\n",
    "\n",
    "    # Train the model\n",
    "    train_start_time = datetime.datetime.now().strftime(\"%I:%M%p on %B %d, %Y\")\n",
    "\n",
    "    history = model.fit(train[FEATURES][:examples_limit],\n",
    "                        train[TARGETS][:examples_limit],\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_split=validation_split,\n",
    "                        validation_data=val,\n",
    "                        verbose=1,\n",
    "                        class_weight=class_weight,\n",
    "                        callbacks=[tb_callback, ckpt_callback])\n",
    "\n",
    "    train_end_time = datetime.datetime.now().strftime(\"%I:%M%p on %B %d, %Y\")\n",
    "\n",
    "    history_filename = os.path.join(log_dir, 'history.json')\n",
    "    info_filename = os.path.join(log_dir, 'info.txt')\n",
    "    \n",
    "    model_filename = os.path.join(log_dir, 'saved_model.h5')\n",
    "    model.save(model_filename)\n",
    "\n",
    "    with open(history_filename, 'w') as file:\n",
    "        json.dump(history.history, file)\n",
    "\n",
    "    with open(info_filename, 'w') as file:\n",
    "        file.write('***Training Info***\\n')\n",
    "        file.write('Training Start: {}'.format(train_start_time))\n",
    "        file.write('Training End: {}\\n'.format(train_end_time))\n",
    "        file.write('Arguments:\\n')\n",
    "        for arg in sys.argv:\n",
    "            file.write('\\t{}\\n'.format(arg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, _ = load_image_h5_colab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf logs/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cnn.train.train(train=train, \n",
    "                log_dir='logs/', \n",
    "                validation_split=0.15,\n",
    "                lr=1e-3, \n",
    "                freeze=True, \n",
    "                examples_limit=160,\n",
    "                epochs=20, \n",
    "                batch_size=32,\n",
    "                seed=71,\n",
    "                decay=0.,\n",
    "                use_dropout=False,\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/ --port 6007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.eval.eval(model_file='cnn/logs/saved_model.h5', data=train, name='CNN')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
