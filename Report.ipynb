{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Author**: Anton SÃ¥mark-Roth <br>\n",
    "Mail: <a href=\"mailto:anton.samark-roth@nuclear.lu.se\">anton.samark-roth@nuclear.lu.se</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Report - AT-TPC project\n",
    "Machine Learning and Data Analysis for Nuclear Physics, a Nuclear TALENT Course at the ECT*, June 22 to July 3 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Report\" data-toc-modified-id=\"Report-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Report</a></span><ul class=\"toc-item\"><li><span><a href=\"#About-this-notebook\" data-toc-modified-id=\"About-this-notebook-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>About this notebook</a></span></li></ul></li><li><span><a href=\"#Examining-the-data\" data-toc-modified-id=\"Examining-the-data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Examining the data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-of-interest---Lifetimes-in-decay-chains\" data-toc-modified-id=\"Data-of-interest---Lifetimes-in-decay-chains-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Data of interest - Lifetimes in decay chains</a></span></li><li><span><a href=\"#Motivation---Do-all-decay-chains-in-a-set-have-a-common-origin?\" data-toc-modified-id=\"Motivation---Do-all-decay-chains-in-a-set-have-a-common-origin?-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Motivation - Do all decay chains in a set have a common origin?</a></span></li></ul></li><li><span><a href=\"#Schmidt-tests-on-the-set-of-short-E115-decay-chains\" data-toc-modified-id=\"Schmidt-tests-on-the-set-of-short-E115-decay-chains-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Schmidt tests on the set of short E115 decay chains</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-Schmidt-test\" data-toc-modified-id=\"The-Schmidt-test-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>The Schmidt test</a></span></li><li><span><a href=\"#Generalised-Schmidt-test\" data-toc-modified-id=\"Generalised-Schmidt-test-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Generalised Schmidt test</a></span></li><li><span><a href=\"#The-generalised-Schmidt-method-applied\" data-toc-modified-id=\"The-generalised-Schmidt-method-applied-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>The generalised Schmidt method applied</a></span><ul class=\"toc-item\"><li><span><a href=\"#Simulation-of-expected-values-and-confidence-intervals\" data-toc-modified-id=\"Simulation-of-expected-values-and-confidence-intervals-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Simulation of expected values and confidence intervals</a></span></li></ul></li><li><span><a href=\"#Summary-Generalised-Schmidt-Method\" data-toc-modified-id=\"Summary-Generalised-Schmidt-Method-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Summary Generalised Schmidt Method</a></span></li></ul></li><li><span><a href=\"#Figure-of-Merit-(FoM)-method\" data-toc-modified-id=\"Figure-of-Merit-(FoM)-method-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Figure-of-Merit (FoM) method</a></span><ul class=\"toc-item\"><li><span><a href=\"#Simulations\" data-toc-modified-id=\"Simulations-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Simulations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Simulating-sets-of-decay-chains\" data-toc-modified-id=\"Simulating-sets-of-decay-chains-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Simulating sets of decay chains</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.-Random-sampling-of-$\\tau$-from-its-likelihood-function\" data-toc-modified-id=\"1.-Random-sampling-of-$\\tau$-from-its-likelihood-function-4.1.1.1\"><span class=\"toc-item-num\">4.1.1.1&nbsp;&nbsp;</span>1. Random sampling of $\\tau$ from its likelihood function</a></span></li><li><span><a href=\"#2.-Simulating-100-000-sets-of-decay-chains-AND-calculating-their-FoM-values.\" data-toc-modified-id=\"2.-Simulating-100-000-sets-of-decay-chains-AND-calculating-their-FoM-values.-4.1.1.2\"><span class=\"toc-item-num\">4.1.1.2&nbsp;&nbsp;</span>2. Simulating 100 000 sets of decay chains AND calculating their FoM values.</a></span></li></ul></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>Summary</a></span></li></ul></li></ul></li><li><span><a href=\"#The-alleged-link-between-Element-117-and-115\" data-toc-modified-id=\"The-alleged-link-between-Element-117-and-115-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>The alleged link between Element 117 and 115</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extracting-data\" data-toc-modified-id=\"Extracting-data-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Extracting data</a></span></li><li><span><a href=\"#(i)-Calculations\" data-toc-modified-id=\"(i)-Calculations-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>(i) Calculations</a></span></li><li><span><a href=\"#(ii)-Simulations\" data-toc-modified-id=\"(ii)-Simulations-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>(ii) Simulations</a></span></li><li><span><a href=\"#Results-and-Conclusions\" data-toc-modified-id=\"Results-and-Conclusions-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Results and Conclusions</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About this notebook\n",
    "The main objective is to classify events aquired from an Active Target (simulated data), as being \"beam\" or \"reaction\" events. The idea is to implement a \"software trigger\" which would be able to effectively select only relevant data to save on disk for future analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated in the writeup, the ML model will quickly determine whether or not a reaction occured -- this can be added into the experiment as a \"software trigger\", which helps write only interesting events to disk.\n",
    "\n",
    "the 22Mg experiment, was a 22Mg beam incident on alpha particles, i.e. He gas in the detector volume.\n",
    "\n",
    "They looked for excited states in the 22Mg+ alpha combined system to better understand the nuclear structure (edited) \n",
    "\n",
    "But, in principle, this model could be used for any future experiment using the AT-TPC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategy\n",
    "We will apply the ML techiques learned during the Talent Course, in order to analyse the data and reach the best performance possible.<br>\n",
    "First there will be a section dedicated to data visualization, and in which we will discuss the general characteristic of the task. Afterwards, we will propose various approaches to tackle to problem, and show the results for each methods. Eventually, we will summarize and discuss the outcomes in the final section.\n",
    "\n",
    "Models used in this project:\n",
    "- Logistic Regression\n",
    "- Random Forest\n",
    "- Dense Neural Networks DNN\n",
    "- K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import h5py\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_processing.data as dp\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport data_processing.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Format\n",
    "Data are saved in the file \"Mg22_alphaalpha_digiSim.h5\", which contains 2000 simulated events: even numbers are \"reaction\" events, while odd numbers are \"beam\" event.<br>\n",
    "Each event is constitued by a point cloud of the active pads for that event.<br>\n",
    "In turn, every hit pad is associated with a 4-tuple (x,y,z,q): x and y are the position of the hit pad on the detector plane, z is the coordinare associated to the beam axis, and q is the charge deposited on the pad.<br> \n",
    "Actually, there is an extra column associated with time (not needed since data already contains the z coordinate thanks to a pre-processing phase), and also an extra column of zeros, unnecessary for this analysis.<br>\n",
    "The number of hit pads may vary for each event, ranging from around 20, up to a few hundred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate structure of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data_processing/data/Mg22_alphaalpha_digiSim.h5'\n",
    "hf = h5py.File(filename, \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_i = 1\n",
    "event = hf[\"Event_[{}]\".format(event_i)][:]\n",
    "display(pd.DataFrame(event))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = []\n",
    "for key in hf.keys():\n",
    "    length.append(len(hf[key]))\n",
    "length = np.asarray(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['font.size'] = 18\n",
    "plt.figure()\n",
    "plt.hist(length, bins=100)\n",
    "plt.xlabel(\"Length (items)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data of main interest to this study is four dimensional, $(x,y,z,A)$. Let's first investigate the full 4D-structure of a beam- and a reaction-labelled event:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_i = 4\n",
    "event_j = event_i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "mpl.rcParams['font.size'] = 16\n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "ax = fig.add_subplot(211, projection='3d')\n",
    "sc = ax.scatter(dp.get_event_by_index(hf, event_i)[\"x\"], dp.get_event_by_index(hf, event_i)[\"y\"], dp.get_event_by_index(hf, event_i)[\"z\"], c=dp.get_event_by_index(hf, event_i)[\"A\"], cmap='inferno')\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_zlabel(\"z\")\n",
    "ax.set_title(\"{}, {}\".format(dp.get_label_name(dp.get_key_from_index(event_i)), dp.get_key_from_index(event_i)))\n",
    "plt.colorbar(sc, ax=ax, label='Deposited charge')\n",
    "\n",
    "ax = fig.add_subplot(212, projection='3d')\n",
    "sc = ax.scatter(dp.get_event_by_index(hf, event_j)[\"x\"], dp.get_event_by_index(hf, event_j)[\"y\"], dp.get_event_by_index(hf, event_j)[\"z\"], c=dp.get_event_by_index(hf, event_j)[\"A\"], cmap='inferno')\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_zlabel(\"z\")\n",
    "#ax.set_xlim(0, gv.X_DISC)\n",
    "#ax.set_ylim(0, gv.Y_DISC)\n",
    "#ax.set_zlim(0, gv.Z_DISC)\n",
    "ax.set_title(\"{}, {}\".format(dp.get_label_name(dp.get_key_from_index(event_j)), dp.get_key_from_index(event_j)))\n",
    "plt.colorbar(sc, ax=ax, label='Deposited charge')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing several pairs of events will show beam and reaction events in general having a significantly different spatial charge distribution. Before narrowing down on the features, the events are also visualised in images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The events may be visualized in images with projections to e.g. the $xy$ plane , where the deposited charge ($A$) is represented by the pixel color. Such images for 16 different events are presented in the figure below (choose which projection in the following code cell). The reaction events tend to have long continuous tracks while the beam events have two $z$-components while being centered around $(x,y)=(0,0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection = 'zy' # 'xy', 'xz', 'zy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "mpl.rcParams['font.size'] = 14\n",
    "\n",
    "n_rows, n_cols = 4, 4\n",
    "\n",
    "on_x = None\n",
    "on_y = None\n",
    "\n",
    "if projection == 'xy':\n",
    "    on_x, on_y = \"x\", \"y\"\n",
    "elif projection == 'xz':\n",
    "    on_x, on_y = \"x\", \"z\"\n",
    "elif projection == 'zy':\n",
    "    on_x, on_y = \"z\", \"y\"\n",
    "else:\n",
    "    print(\"Invalid projection:\", projection)\n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "\n",
    "for i in range(n_rows*n_cols):\n",
    "    ax = plt.subplot(n_rows, n_cols, i+1)\n",
    "    sc = plt.scatter(dp.get_event_by_index(hf, i)[on_x], dp.get_event_by_index(hf, i)[on_y], c=np.log(dp.get_event_by_index(hf, i)[\"A\"]), cmap='Greys')\n",
    "    ax.set_xlabel(on_x)\n",
    "    ax.set_ylabel(on_y)\n",
    "    ax.set_title(\"{}, {}\".format(dp.get_label_name(dp.get_key_from_index(i)), dp.get_key_from_index(i)))\n",
    "    #plt.xlim(-275.0, 275.0)\n",
    "    #plt.ylim((-275.0, 275.0))\n",
    "    #ax.set_aspect('equal', adjustable='box')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logarithmic scaling of the charge deposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For eventual further processing of such data in machine learning models, it is wise to use a grey color map. Doing so will render parts of the detected track in the event invisible (or close to) using a linear scale, see left panel in the figure below. In this case, important features will be missing or largely suppressed. This motivates a transformation of the deposited charge to logarithmic scale, see right panel of figure below (already used for the figure above). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_i = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "mpl.rcParams['font.size'] = 18\n",
    "\n",
    "fig = plt.figure(figsize=(18,6))\n",
    "ax = plt.subplot(131)\n",
    "sc = plt.scatter(dp.get_event_by_index(hf, event_i)[\"x\"], dp.get_event_by_index(hf, event_i)[\"y\"], c=dp.get_event_by_index(hf, event_i)[\"A\"], cmap='Greys')\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_title(\"XY projection\\n linear scale\")\n",
    "cbar = plt.colorbar(sc, ax=ax, orientation='vertical', label='A')    \n",
    "\n",
    "ax = plt.subplot(132)\n",
    "sc = plt.scatter(dp.get_event_by_index(hf, event_i)[\"x\"], dp.get_event_by_index(hf, event_i)[\"y\"], c=np.log(dp.get_event_by_index(hf, event_i)[\"A\"]), cmap='Greys')\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"z\")\n",
    "ax.set_title(\"XY projection\\n log scale\")\n",
    "cbar = plt.colorbar(sc, ax=ax, orientation='vertical', label='log(A)')    \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "mpl.rcParams['font.size'] = 18\n",
    "\n",
    "event_i = 5\n",
    "\n",
    "fig = plt.figure(figsize=(18,6))\n",
    "ax = plt.subplot(131)\n",
    "sc = plt.scatter(dp.get_event_by_index(hf, event_i)[\"x\"], dp.get_event_by_index(hf, event_i)[\"y\"], c=np.log(dp.get_event_by_index(hf, event_i)[\"A\"]), cmap='Greys')\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_title(\"XY projection\")\n",
    "\n",
    "ax = plt.subplot(132)\n",
    "sc = plt.scatter(dp.get_event_by_index(hf, event_i)[\"x\"], dp.get_event_by_index(hf, event_i)[\"z\"], c=np.log(dp.get_event_by_index(hf, event_i)[\"A\"]), cmap='Greys')\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"z\")\n",
    "ax.set_title(\"XZ projection\")\n",
    "\n",
    "ax = plt.subplot(133)\n",
    "sc = plt.scatter(dp.get_event_by_index(hf, event_i)[\"z\"], dp.get_event_by_index(hf, event_i)[\"y\"], c=np.log(dp.get_event_by_index(hf, event_i)[\"A\"]), cmap='Greys')\n",
    "ax.set_xlabel(\"z\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_title(\"ZY projection\")\n",
    "\n",
    "cbar = fig.colorbar(sc, orientation='vertical', label='log(A)')    \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important features to discriminate beam from reaction events have already been determined from the visualisations above. For a complete overview of the data set, the mean of $(x,y,z)$ as well as the sum of the deposited charge ($A$) for each event is calculated. Distributions and correlations of the parameters are then investigated in the pair plot below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following observations are made:\n",
    "\n",
    "* Beam events are strikingly centered about $(x,y) = (0,0)$, while reaction events are wildly scattered across the $xy$-plane. The standard deviation of each coordinate (see output of next hidden cell) further shows this.\n",
    "* Reaction events tend to assume smaller $z$-values and generally deposit a larger energy ($A$) for each event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cols = [\"Event\", \"Label\", \"length\", \n",
    "        \"x_mean\", \"y_mean\", \"z_mean\", \"A_sum\",\n",
    "        \"x_std\", \"y_std\", \"z_std\",\n",
    "       ]\n",
    "df = pd.DataFrame(columns=cols)\n",
    "for i, key in enumerate(hf.keys()):\n",
    "#for i, key in enumerate(sorted(dict_data)):\n",
    "    #if i > 3:\n",
    "    #    break\n",
    "    #print(i, key)\n",
    "    #print(pd.DataFrame(hf[key][:]))\n",
    "    d = pd.DataFrame(hf[key][:])\n",
    "    means = np.mean(d, axis=0)\n",
    "    sums = np.sum(d, axis=0)\n",
    "    std_devs = np.std(d, axis=0)\n",
    "\n",
    "    #print(means)\n",
    "    df = df.append({\"Event\": dp.get_event_from_key(key), \"Label\": dp.get_label_name(key), \"length\": d.shape[0], \n",
    "                    \"x_mean\": means[\"x\"], \"y_mean\": means[\"y\"], \"z_mean\": means[\"z\"], \"A_sum\": sums[\"A\"],\n",
    "                    \"x_std\": std_devs[\"x\"], \"y_std\": std_devs[\"y\"], \"z_std\": std_devs[\"z\"],\n",
    "                   }, ignore_index=True)\n",
    "df = df.sort_values(\"Event\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(context='notebook', style='darkgrid', palette='deep', font_scale=2., color_codes=True, rc=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['font.size'] = 18\n",
    "sns.pairplot(df, hue=\"Label\", vars=[\"x_mean\", \"y_mean\", \"z_mean\", \"A_sum\"], kind='scatter', diag_kind='hist');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "sns.pairplot(df, hue=\"Label\", vars=[\"x_std\", \"y_std\", \"z_std\", \"A_sum\"], kind='scatter', diag_kind='hist');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take-aways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All $(x,y,z,A)$ parameters show features which could enable a distinction between beam and reaction events. I.e. all parameters could be part of the input features provided to the machine learning models. \n",
    "\n",
    "Concerning the image representation of the event data, it is not clear whether one projection should be preferred over the others.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "_Or how to present the data to the models_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression and neural network models require the input-feature data to have fixed length. The AT-TPC detector enables a resolution of over 5 million voxels across its volume. Having each event being represented with a feature vector of such length is not feasible, therefore the resolution is decreased considerably through two discretization procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The event data is either discretized into voxels or pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voxels\n",
    "\n",
    "Voxels are provided as the input data to the logistic regression and the fully-connected neural network models. Each dimension of the detector is divided into 20 sub parts creating in total $20\\times20\\times20=8000$ voxels. In this process, the `log10` of the deposited charge on each pad is summed within the volume of each voxel. The input data is a 1D vector of length 8000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**See also**: [data_processing/generate_voxels.py](./data_processing/generate_voxels.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the discretization process, i.e. converting raw data to voxels, is illustrated for one event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import data_processing.generate_voxels as gv\n",
    "%aimport data_processing.generate_voxels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_i = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#discretize voxels\n",
    "xyzs = gv.discretize_grid_charge(dp.get_event_by_index(hf, event_i), 20, 20, 20)\n",
    "voxels = xyzs.toarray()\n",
    "\n",
    "#from voxel to (x,y,z) coordinate\n",
    "mask = voxels[0] > 0\n",
    "bucket_num = np.array(range(len(voxels[0])))[mask]\n",
    "x, y, z = gv.get_xyz_from_bucket(bucket_num, 20, 20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['font.size'] = 16\n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "ax = fig.add_subplot(211, projection='3d')\n",
    "sc = ax.scatter(dp.get_event_by_index(hf, event_i)[\"x\"], dp.get_event_by_index(hf, event_i)[\"y\"], dp.get_event_by_index(hf, event_i)[\"z\"], c=dp.get_event_by_index(hf, event_i)[\"A\"], cmap='inferno', marker='.')\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_zlabel(\"z\")\n",
    "ax.set_title(\"Raw data\")\n",
    "plt.colorbar(sc, ax=ax, label='Deposited charge')\n",
    "\n",
    "ax = fig.add_subplot(212, projection='3d')\n",
    "sc = ax.scatter(x, y, z, c=voxels[0][mask], cmap='inferno', marker='s')\n",
    "ax.set_xlabel(\"x (bin)\")\n",
    "ax.set_ylabel(\"y (bin)\")\n",
    "ax.set_zlabel(\"z (bin)\")\n",
    "ax.set_title(\"Discretized data\")\n",
    "plt.colorbar(sc, ax=ax, label='Deposited charge (scaled)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pixels\n",
    "\n",
    "The input data for the CNN model expects a format of images. These images are generated by first performing a `log10` scaling and normalizing with respect to a maximum charge, determined based on the full training data set, of the deposited charge on each pad. The projection of the raw data is plotted in a scatterplot with dimensions being that of the AT-TPC detector where the figure size is $1''\\times1''$ and has a `dpi=48` (dots per inch). The figure is rendered using the matplotlib canvas renderer into a $48\\times48$ pixel image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**See also**: [data_processing/generate_images.py](./data_processing/generate_images.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the discretization process, i.e. converting raw data to images, is illustrated for one event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import data_processing.generate_images as gi\n",
    "%aimport data_processing.generate_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_i = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "data = [[np.asarray(pd.DataFrame(dp.get_event_by_index(hf, event_i))), dp.get_label(dp.get_key_from_index(event_i))]]\n",
    "\n",
    "#print(\"Shape:\\n\\tdata:\", len(data))\n",
    "data, max_charge = gi.transform_data(data)\n",
    "\n",
    "features, targets = gi.make_image_features_targets(data, 'xy', 48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "mpl.rcParams['font.size'] = 18\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "sc = plt.scatter(dp.get_event_by_index(hf, event_i)[\"x\"], dp.get_event_by_index(hf, event_i)[\"y\"], c=np.log(dp.get_event_by_index(hf, event_i)[\"A\"]), cmap='Greys', )\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_title(\"Raw data\")\n",
    "plt.xlim(-275.0, 275.0)\n",
    "plt.ylim((-275.0, 275.0))\n",
    "cbar = fig.colorbar(sc, orientation='vertical', label='log(A)')\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "im = ax.imshow(features[0], cmap='Greys')\n",
    "ax.set_xlabel(\"x (bin)\")\n",
    "ax.set_ylabel(\"y (bin)\")\n",
    "ax.set_title(\"Discretized data\")\n",
    "plt.colorbar(im, ax=ax, label=\"RGB\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated voxel data, which provides the input features to the logistic regression and the fully-connected neural network models, are normalized by the Euclidean norm with `sklearn.preprocessing.Normalizer`. As the `VGG16` pre-trained network uses RGB images as input, no normalization is applied to the images which are the input to the convolutional neural network model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train (0.8) and test (0.2) data sets were generated and written to disk, to be loaded during the later training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**See also**: [GenerateDataSets.ipynb](./GenerateDataSets.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**See**: [LogisticTraining.ipynb](./LogisticTraining.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FCNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**See**: [FCNNTraining.ipynb](./FCNNTraining.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to the FCNN, a small subset of the data was initially trained while freezing the pre-trained weights of the `VGG16` net and staying away from any regularization. \n",
    "First of all, the network was assured to learn with the training, i.e. the loss decreased with the number of epochs in the training.\n",
    "\n",
    "Computations were quickly moved to _Google Colab_ as they took too much time on my laptop, see [https://colab.research.google.com/github/ASamarkRoth/Machine-Learning-Project-ECT-Talent/blob/master/CNNTraining-Colab.ipynb#scrollTo=cEC7P4zB29KT](https://colab.research.google.com/github/ASamarkRoth/Machine-Learning-Project-ECT-Talent/blob/master/CNNTraining-Colab.ipynb#scrollTo=cEC7P4zB29KT) In the colab framework, the training could with ease be applied to the complete data set.\n",
    "\n",
    "A grid of [1e-3, 5e-5] learning rate values were explored. Trends in the loss function were studied. A learning rate of 1e-4, that rather smoothly improved the loss as the number of epochs increased, was chosen, see also the following image:\n",
    "\n",
    "![](./images/CNN_learning_rate.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import fcnn.eval\n",
    "import cnn.eval\n",
    "import logistic.eval\n",
    "import data_processing.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Loading test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "data_dir = \"./data_processing/voxels/\"\n",
    "# Load data\n",
    "_, test_voxels = data_processing.data.load_discretized_data(data_dir, prefix='Grid20', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "path_data = './data_processing/images/xyimages.h5'\n",
    "_, test_images = data_processing.data.load_image_h5(path_data, categorical=False, binary=True, reverse_labels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "The classification report along with the confusion matrix of the logistic regression model with $l^1$ regularization based on the test data can be found below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic.eval.eval(model_file='logistic/logs/logistic_cv_model.pkl', data=test_voxels, name=\"LogisticCVRegression\", examples_limit=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FCNN\n",
    "\n",
    "The classification report along with the confusion matrix of the fully-connected neural network based on the test data can be found below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcnn.eval.eval(model_file='fcnn/logs/nodes128_dropoutTrue_lr0.001_decay0.0_samples-1_final_saved_model.h5', data=test_voxels, name='FCNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "\n",
    "The classification report along with the confusion matrix of the convolutional neural network based on the test data can be found below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.eval.eval(model_file='fcnn/logs/nodes128_dropoutTrue_lr0.001_decay0.0_samples-1/20200826-093241/saved_model.h5', data=test_voxels, name='FCNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Simplest model performing the best should be used\n",
    "* Next step, try the models on experimental data. Does simulated data include noise, otherwise that could also be an idea.\n",
    "* Which accuracy is sufficient? For experimental case\n",
    "* Is the data set representative?\n",
    "* In the experiment how is the model implemented? It is a bit unclear for me which parameters are important? E.g. computational performance, memory, ease of implementation?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "livereveal": {
   "scroll": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "567px",
    "left": "0px",
    "right": "1108px",
    "top": "111px",
    "width": "193px"
   },
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
